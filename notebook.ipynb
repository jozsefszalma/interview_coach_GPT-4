{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Interview Coach\n",
    "### Mock Interview with Feedback\n",
    "\n",
    "### LICENSE AND DISCLAIMER:\n",
    "Copyright 2023, Jozsef Szalma<br>\n",
    "Creative Commons Attribution-NonCommercial 4.0 International Public License <br>\n",
    "Gradio code was partially reused from / informed by [this guide](https://www.gradio.app/guides/creating-a-chatbot-fast)\n",
    "\n",
    "Before repurposing this code for an HR use-case consider: <br>\n",
    "* OpenAI's [useage policies](https://openai.com/policies/usage-policies) expliclitly prohibit:<br>\n",
    "\"Activity that has high risk of economic harm, including [...] Automated determinations of eligibility for [...] employment [...]\" <br>\n",
    "\n",
    "* The [EU AI Act proposal](https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF) contains the following language:<br>\n",
    "\"AI systems used in employment, workers management and access to self-employment,\n",
    "notably for the <b>recruitment and selection of persons</b> [..] should also be <b>classified as high-risk</b>\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### KNOWN ISSUES:\n",
    "* incomplete error handling around job description, e.g. if an invalid JD URL is provided the code won't fall back to the copy-pasted JD\n",
    "* if no JD and/or CV are provided GPT-4 might on occasion ignore instructions to only ask one interview question at a time\n",
    "* the current workflow consumes a lot of tokens as the JD and the CV aren't summarized, but considered as-is for each question\n",
    "* the scraping logic breaks once the job is in the \"no longer accepting applications\" status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies, assuming Python 3.9.16\n",
    "%pip install openai==0.27.8\n",
    "%pip install ipykernel==6.25.1\n",
    "%pip install gradio==3.40.1\n",
    "%pip install requests==2.31.0\n",
    "%pip install beautifulsoup4==4.12.2\n",
    "%pip install pdfplumber==0.10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: set up env variable \"KEY\" for your openai API key (e.g. in a .env file if using vscode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env variables\n",
    "import os\n",
    "\n",
    "#API\n",
    "import openai\n",
    "\n",
    "#UI\n",
    "import gradio as gr\n",
    "\n",
    "#to digest the Job Description and the Resume\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "import io\n",
    "import re\n",
    "\n",
    "#store OpenAI API key in .env file or replace right side of the equation with your key\n",
    "openai.api_key = os.getenv(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI Parameters\n",
    "#I'm using two prompts here, immagine this like a two person interview panel, one conducts the interview, the other evaluates\n",
    "\n",
    "INTERVIEWER_MODEL = 'gpt-4-0613'\n",
    "INTERVIEWER_TEMPERATURE = 0.4\n",
    "INTERVIEWER_TOKEN_LIMIT = 300\n",
    "\n",
    "INTERVIEWER_PROMPT = \"\"\"\n",
    "    Role: \n",
    "        Interviewer in a job interview coaching application; your role is to interview the candidate. \n",
    "        Do not provide feedback, that is done after the interview by a human.\n",
    "        Follow the interview script, don't ask more than one question per message.\n",
    "\n",
    "    Interview script:\n",
    "        1) Welcome the candidate\n",
    "        2) Check if a CV was automatically provided by the system, ask the candidate to provide their CV if not.\n",
    "        3) If the CV was provided by the system ask the candidate to confirm if you have their correct CV by showing a short summary.\n",
    "        4) Check if a Job Description was automatically provided by the system, ask the candidate to provide the JD they are interviewing for if not. \n",
    "        5) If the JD was provided by the system ask the candidate to confirm if you have the correct JD by showing a short summary.\n",
    "        6) Compare and contrast Candidate Resume and Job Description and ask the first clarification question from the candidate to establish overlaps and disconnects between JD and CV.\n",
    "        7) Ask the 2nd clarification question from the candidate to establish overlaps and disconnects between JD and CV.\n",
    "        8) Ask the 3rd clarification question from the candidate to establish overlaps and disconnects between JD and CV.\n",
    "        9) Ask the candidate for their motivation to apply to this job, if not yet discussed.\n",
    "        10) Thank the candidate, explain that feedback will be provided at a later stage and append to your last message {interview ended}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "REVIEWER_MODEL = 'gpt-4-0613'\n",
    "REVIEWER_TEMPERATURE = 0.2\n",
    "REVIEWER_TOKEN_LIMIT = 2000\n",
    "\n",
    "REVIEWER_PROMPT = \"\"\"\n",
    "    Role:\n",
    "        Job interview coach in a job interview coaching application.\n",
    "\n",
    "    Task:\n",
    "        Your role is to review a conversation between the interviewer and the candidate and provide feedback.\n",
    "        Only consider job relevant questions, additional chatter (e.g. confirming data) can be ignored.\n",
    "        Rate answers on a scale from 1 (worst) to 10 (best).\n",
    "        Recommend an alternative answer for each question.\n",
    "        Provide your response as a valid, but human readable JSON, see template:\n",
    "\n",
    "        {\n",
    "            \"questions\": [\n",
    "                {\n",
    "                    \"question_number\": 1,\n",
    "                    \"question_text\": \"Could you please ellaborate on...\",\n",
    "                    \"candidate_answer\": \"I think...\",\n",
    "                    \"recommended_answer\": \"\",\n",
    "                    \"answer_correctness_rating\": 9\n",
    "                }\n",
    "            ],\n",
    "            \"overall_rating\": \"90%\"\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chatbot logic\n",
    "I recommend to run this either from docker or <br>\n",
    "if running from an IDE then connect to the Local URL listed in the output, Gradio's default http://127.0.0.1:7860 <br>\n",
    "as the UI might not get fully rendered in a small window inside an IDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching additional inputs \n",
    "linkedin_jd_cache = {}\n",
    "linkedin_jd = \"\"\n",
    "candidate_cv = \"\"\n",
    "\n",
    "#For the sake of simplicity I'm providing an option to scrape the JD from LinkedIn directly \n",
    "#This is probably against LinkedIn's T&Cs, so use at your own risk\n",
    "#Also, the scraping logic breaks once the job is in the \"no longer accepting applications\" status\n",
    "def extract_linkedin_jd (URL, copy_paste):\n",
    "    global linkedin_jd_cache\n",
    "    global linkedin_jd\n",
    "    if URL:\n",
    "        #Checking if the JD has already been scraped\n",
    "        if URL in linkedin_jd_cache:\n",
    "            print(f\"Text for {URL} already loaded.\")\n",
    "            return linkedin_jd_cache[URL]\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(URL, headers=headers)\n",
    "        jd = \"\"\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            #Finding the job title\n",
    "            job_title_tag = soup.find('h1', class_='topcard__title')\n",
    "            if job_title_tag:\n",
    "                job_title = job_title_tag.text.strip()\n",
    "                jd = jd + job_title\n",
    "            else:\n",
    "                jd = jd + \"Couldn't find job title on LinkedIn \\n\"\n",
    "\n",
    "            #Finding the company name\n",
    "            company_name_tag = soup.find('a', class_='topcard__org-name-link')\n",
    "            if company_name_tag:\n",
    "                company_name = company_name_tag.text.strip()\n",
    "                jd = jd + company_name\n",
    "            else:\n",
    "                jd = jd + \"Couldn't find company name on LinkedIn \\n\"\n",
    "\n",
    "            # Finding the job description\n",
    "            job_description_tag = soup.find('div', class_='description__text')\n",
    "            if job_description_tag:\n",
    "                job_description = job_description_tag.text.strip()\n",
    "                jd = jd + job_description\n",
    "            else:\n",
    "                jd = jd + \"Couldn't find job description on LinkedIn \\n\"\n",
    "            jd = re.sub('\\n+', '\\n', jd)\n",
    "            linkedin_jd_cache[URL] = jd\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "            jd = \"couldn't load JD from LinkedIn\"\n",
    "\n",
    "    elif copy_paste:\n",
    "        jd = copy_paste\n",
    "    \n",
    "    else:\n",
    "        jd = \"no JD provided\"\n",
    "\n",
    "    linkedin_jd = jd\n",
    "    return jd\n",
    "\n",
    "\n",
    "#Loading the Candidate's Resume\n",
    "def load_cv (cv_pdf):\n",
    "    global candidate_cv\n",
    "\n",
    "    #converting between Gradio's feed and what pdfpluber can digest\n",
    "    cv_pdf = io.BytesIO(cv_pdf)\n",
    "    \n",
    "    with pdfplumber.open(cv_pdf) as pdf:\n",
    "        #initializing an empty string to store the extracted text\n",
    "        text = \"\"\n",
    "\n",
    "        #iterating over each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            #extracting text from the page and add it to the text string\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        #removing extra line breaks\n",
    "        text = re.sub('\\n+', '\\n', text)\n",
    "        \n",
    "        candidate_cv = text\n",
    "\n",
    "        return text\n",
    "\n",
    "#extracting control messages from streaming text enclosed in curly braces\n",
    "#this can be used, inter alia, for the interviewer to indicate the end of the interview\n",
    "#TODO replace this with a more formalized solution detailed here: https://platform.openai.com/docs/guides/gpt/function-calling\n",
    "def extract_control(text):\n",
    "        \n",
    "    pattern = r'{(.*?)}'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        control = match.group().replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "        text_without_control = re.sub(pattern, '', text)\n",
    "    else:\n",
    "        control_start = text.find('{')\n",
    "        if control_start != -1:\n",
    "            control = text[control_start + 1:]\n",
    "            text_without_control = text[:control_start]\n",
    "        else:\n",
    "            control = \"\"\n",
    "            text_without_control = text\n",
    "\n",
    "    return text_without_control, control\n",
    "\n",
    "#this is the handler function that gets triggered when the submit button is pressed\n",
    "#contains the prompt engineering logic as well\n",
    "def btn_handler(message, history): \n",
    "    \n",
    "    #this part handles the standard interview\n",
    "    history_openai_format = []\n",
    "    #adding the interviewer prompt as a system message\n",
    "    history_openai_format.append({\"role\": \"system\", \"content\": INTERVIEWER_PROMPT})\n",
    "    #adding the JD as a system message\n",
    "    history_openai_format.append({\"role\": \"system\", \"content\": \"Job Description: \" + linkedin_jd})\n",
    "    #adding the CV as a system message\n",
    "    history_openai_format.append({\"role\": \"system\", \"content\": \"Candidate's CV: \" + candidate_cv})\n",
    "    #translating the Gradio chat history into OpenAI format\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    #submitting the interviewer inference request to the API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = INTERVIEWER_MODEL,\n",
    "        messages = history_openai_format,\n",
    "        temperature = INTERVIEWER_TEMPERATURE,\n",
    "        max_tokens  = INTERVIEWER_TOKEN_LIMIT,\n",
    "        stream = True,\n",
    "        n = 1\n",
    "    )\n",
    "\n",
    "    partial_message = \"\"\n",
    "    control_message = \"\"\n",
    "    trimmed_message = \"\"\n",
    "    #yielding the streamed message to the chat window, while ensuring control messages don't become visible\n",
    "    for chunk in response:\n",
    "        if len(chunk['choices'][0]['delta']) != 0:\n",
    "            partial_message = partial_message + chunk['choices'][0]['delta']['content']\n",
    "            trimmed_message, control_message = extract_control (partial_message)\n",
    "            yield trimmed_message\n",
    "\n",
    "\n",
    "    #if the interview has ended let's ask for the evaluation\n",
    "    if control_message == \"interview ended\":\n",
    "        print(\"starting eval\")\n",
    "\n",
    "        eval_prompt = []\n",
    "        history_adj_format = []\n",
    "        #composing the eval prompt\n",
    "        eval_prompt.append({\"role\": \"system\", \"content\": REVIEWER_PROMPT})\n",
    "        eval_prompt.append({\"role\": \"system\", \"content\": \"Job Description: \" + linkedin_jd})\n",
    "        eval_prompt.append({\"role\": \"system\", \"content\": \"Candidate's CV: \" + candidate_cv})\n",
    "        \n",
    "        #transforming the chat history to ensure the reviewer model don't get confused and continue the interview\n",
    "        for human, assistant in history:\n",
    "            history_adj_format.append({\"role\": \"candidate\", \"content\": human })\n",
    "            history_adj_format.append({\"role\": \"interviewer\", \"content\":assistant})\n",
    "        eval_prompt.append({\"role\": \"system\", \"content\": str(history_adj_format).replace(\"\\\"\",\"\")})\n",
    "\n",
    "        #making the review inference call to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model = REVIEWER_MODEL,\n",
    "            messages = eval_prompt,\n",
    "            temperature = REVIEWER_TEMPERATURE,\n",
    "            max_tokens  = REVIEWER_TOKEN_LIMIT,\n",
    "            stream = True,\n",
    "            n = 1\n",
    "        )\n",
    "\n",
    "        #continuing the streaming where we left off\n",
    "        partial_message = trimmed_message + r\"<br>\"\n",
    "        for chunk in response:\n",
    "            if len(chunk['choices'][0]['delta']) != 0:\n",
    "                partial_message = partial_message + chunk['choices'][0]['delta']['content']            \n",
    "                yield partial_message   \n",
    "\n",
    "\n",
    "chat_tab = gr.ChatInterface(btn_handler).queue()\n",
    "\n",
    "jd_tab = gr.Interface(\n",
    "    fn=extract_linkedin_jd, \n",
    "    inputs=[\n",
    "            gr.Textbox(\"\", label=\"Job Description LinkedIn URL\"),\n",
    "            gr.Textbox(\"\", label=\"or copy-paste Job Description here\")\n",
    "        ], \n",
    "    outputs=[\n",
    "            gr.Textbox()\n",
    "        ],\n",
    "    allow_flagging=\"never\"\n",
    "    )\n",
    "\n",
    "cv_tab = gr.Interface(\n",
    "    fn=load_cv, \n",
    "    inputs=[\n",
    "            gr.File(type='binary'),\n",
    "        ], \n",
    "    outputs=[\n",
    "            gr.Textbox()\n",
    "        ],\n",
    "    allow_flagging=\"never\"\n",
    "    )\n",
    "\n",
    "\n",
    "demo = gr.TabbedInterface([cv_tab, jd_tab, chat_tab], [\"CV Upload\", \"Job Description\", \"Interview\"]).queue()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
